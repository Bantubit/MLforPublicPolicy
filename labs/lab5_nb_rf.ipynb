{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Naive Bayes and Random Forests\n",
    "In this lab we'll learn to use [GridSearchCV] and take a closer look the [Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB) and [Random Forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes widely known for being effective for spam email classification. We'll try our hand at applying NB on the [Spam email dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from the UCI data repository. Download the dataset and move it to your lab data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  class                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'data/SMSSpamCollection'\n",
    "df = pd.read_table(fname, names=['class', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes\n",
    "Given collection of emails $d^{(1)}, d^{(2)}, ..., d^{(N)}$ with corresponding labels $y^{(i)}$, we first need to extract features from each document. We'll do this by computing word counts of each word in each document. This is commonly referred to as a [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model) representation.\n",
    "\n",
    "After doing some preprocessing of our data, we'll have: $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^N$, where:\n",
    "* $x^{(i)} \\in R^d$ is a word count feature vector for document $i$, and $d$ is the size of the vocabulary. For the purposes of this lab, we define our vocabulary to be the set of words that appear in our dataset of documents.\n",
    "$$ x^{(i)} = (\\theta^{(i)}_1, \\theta^{(i)}_2, ..., \\theta^{(i)}_d)$$\n",
    "$$\\theta^{(i)}_k = \\text{ the number of times word $k$ appears in document $i$}$$\n",
    "\n",
    "\n",
    "* $y^{(i)} \\in \\{0, 1\\}$ is its class, where $0$ denotes spam, and $1$ denotes real email.\n",
    "\n",
    "First we need to compute word count feature vectors. We can do this manually like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def get_all_words(docs):\n",
    "    '''\n",
    "    docs: pandas.Series of strings\n",
    "    Returns: set of words(strings)\n",
    "    '''\n",
    "    # Compute all the words in the docs\n",
    "    words = set()\n",
    "    for d in docs:\n",
    "        # this will strip punctuation\n",
    "        # ref: https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "        d = d.translate(None, string.punctuation)\n",
    "        words = words.union(set(d.split()))\n",
    "    return words\n",
    "\n",
    "def make_word_cnt_feats(docs, vocabulary=None):\n",
    "    '''\n",
    "    docs: iterable(ie: list or pandas.Series) of strings\n",
    "    vocabulary: set of words(strings) that appear in docs\n",
    "    Returns: numpy matrix of the word count features\n",
    "    '''\n",
    "    if vocabulary is None:\n",
    "        vocabulary = get_all_words(docs)\n",
    "    words = sorted(vocabulary)\n",
    "    word_dict = {w: idx for idx, w in enumerate(words)}\n",
    "    \n",
    "    # feature matrix will be of size n x (size of vocabulary + 1)\n",
    "    # add 1 to account for words that dont appear in the vocabulary to avoid random issues\n",
    "    vocab_size = len(words)\n",
    "    word_cnt_features = np.zeros((len(docs), len(words) + 1))\n",
    "    \n",
    "    for idx in range(len(docs)):\n",
    "        doc = docs[idx]\n",
    "        doc = doc.translate(None, string.punctuation)\n",
    "        words = doc.split()\n",
    "        for w in words:\n",
    "            word_idx = word_dict.get(w, vocab_size)\n",
    "            word_cnt_features[idx, word_idx] += 1\n",
    "            \n",
    "    return word_cnt_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape: (5572, 11748)\n"
     ]
    }
   ],
   "source": [
    "vocabulary = get_all_words(df['text'])\n",
    "feature_matrix = make_word_cnt_feats(df['text'], vocabulary)\n",
    "print('Feature matrix shape: {}'.format(feature_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that sklearn actually has a built-in method to do this kind of word count featurization! (But of course, it's good to get your hands dirty with manual featurization from time to time to understand all the ingredients of these algorithms). See sklearn's [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) function for more details.                                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5572x8713 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 74169 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "word_cnt_feats = count_vectorizer.fit_transform(df['text'])\n",
    "word_cnt_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) has a bunch of useful functions that you should checkout. The important ones that we'll use are:\n",
    "* [fit](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit): this learns the vocabulary of the input collection of strings/\"documents\"\n",
    "* [transform](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform): given a collection of documents, this produces the word count features for the documents\n",
    "* fit_transform: calls fit, then transform\n",
    "\n",
    "Note that the word count feature matrix(aka document-term matrix) is in sparse format. This is not an issue for the sklearn models. But if you want to expand them to non-sparse numpy arrays, use the toarray function. Note that this can be problematic for bigger datasets! We can somewhat get around this b/c this dataset is small enough that we can fully expand a 5k by 8k matrix(since it's sparse enough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = word_cnt_feats.toarray()\n",
    "Y = (df['class'] == 'ham')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data in the document-term matrix format, we can use the Naive Bayes classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X_train, y_train)\n",
    "preds = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.983\n",
      "F1 Score: 0.990\n",
      "ROC AUC Score: 0.959\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.3f}'.format(accuracy_score(preds, y_test)))\n",
    "print('F1 Score: {:.3f}'.format(f1_score(preds, y_test)))\n",
    "print('ROC AUC Score: {:.3f}'.format(roc_auc_score(preds, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the code above fixed alpha to 1. But as we've learned in class and last week's lab, we should use cross validation for any kind of parameter search. Last week we wrote code to manually compute model performance across folds to better understand the full cross validation process. Now that we have that under our belt, going foward, we'll simply use sklearn's [GridSearchCV class](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), which will do most of this cumbersome work for us.\n",
    "\n",
    "GridSearchCV requires:\n",
    "* an estimator object(IE: LogisticRegression(), SVC(), KNeighborsClassifier(), etc)\n",
    "* param_grid: a dictionary mapping parameter names to lists of values to search over. We'll see an example below\n",
    "* scoring: a string. See the valid inputs here: http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "* n_jobs: number of jobs to run in parallel to speed up the computation. GridSearchCV can take a long time if you have a lot of different parameter combinations to try out. So it's important to set this parameter if your laptop/desktop has a lot of cores that you want to take advantage of.\n",
    "* cv: int, number of cross validation folds to use\n",
    "* refit: boolean, if true, at the end of the Grid Search, GridSearchCV will refit the entire data with a model using the optimal parameters(you can then access this with the GridSearchCV.best\\_parameters_ attribute).\n",
    "\n",
    "Here's an example of how we'd use it for some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def sample_cross_val_param_search():\n",
    "    # parameters to search over\n",
    "    penalties = ['l1', 'l2']\n",
    "    c_vals = [10**i for i in range(-2, 3)]\n",
    "    param_grid = {'penalty': penalties, 'C': c_vals}\n",
    "    # make some fake data\n",
    "    n_samples = 1000\n",
    "    feats = 10\n",
    "    x_train = np.random.random((n_samples, feats))\n",
    "    y_train = (np.random.random(n_samples) > 0.5) # randomly generate True/False labels\n",
    "    \n",
    "    grid = GridSearchCV(LogisticRegression(), param_grid, 'f1', cv=5)\n",
    "    grid.fit(x_train, y_train)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.01}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.510124</td>\n",
       "      <td>0.536665</td>\n",
       "      <td>0.01</td>\n",
       "      <td>l2</td>\n",
       "      <td>{u'penalty': u'l2', u'C': 0.01}</td>\n",
       "      <td>8</td>\n",
       "      <td>0.505495</td>\n",
       "      <td>0.522546</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431818</td>\n",
       "      <td>0.546433</td>\n",
       "      <td>0.587678</td>\n",
       "      <td>0.567442</td>\n",
       "      <td>0.547368</td>\n",
       "      <td>0.529801</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.053982</td>\n",
       "      <td>0.018286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>l1</td>\n",
       "      <td>{u'penalty': u'l1', u'C': 0.1}</td>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time  mean_test_score  mean_train_score param_C  \\\n",
       "0       0.001166         0.001173         0.000000          0.000000    0.01   \n",
       "1       0.001685         0.001012         0.510124          0.536665    0.01   \n",
       "2       0.001067         0.000889         0.000000          0.000000     0.1   \n",
       "\n",
       "  param_penalty                           params  rank_test_score  \\\n",
       "0            l1  {u'penalty': u'l1', u'C': 0.01}                9   \n",
       "1            l2  {u'penalty': u'l2', u'C': 0.01}                8   \n",
       "2            l1   {u'penalty': u'l1', u'C': 0.1}                9   \n",
       "\n",
       "   split0_test_score  split0_train_score       ...         split2_test_score  \\\n",
       "0           0.000000            0.000000       ...                  0.000000   \n",
       "1           0.505495            0.522546       ...                  0.431818   \n",
       "2           0.000000            0.000000       ...                  0.000000   \n",
       "\n",
       "   split2_train_score  split3_test_score  split3_train_score  \\\n",
       "0            0.000000           0.000000            0.000000   \n",
       "1            0.546433           0.587678            0.567442   \n",
       "2            0.000000           0.000000            0.000000   \n",
       "\n",
       "   split4_test_score  split4_train_score  std_fit_time  std_score_time  \\\n",
       "0           0.000000            0.000000      0.000121        0.000094   \n",
       "1           0.547368            0.529801      0.000080        0.000065   \n",
       "2           0.000000            0.000000      0.000437        0.000030   \n",
       "\n",
       "   std_test_score  std_train_score  \n",
       "0        0.000000         0.000000  \n",
       "1        0.053982         0.018286  \n",
       "2        0.000000         0.000000  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = sample_cross_val_param_search()\n",
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best estimator: LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best Parameters: {'penalty': 'l2', 'C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print('Best estimator: {}'.format(grid.best_estimator_))\n",
    "print('Best Parameters: {}'.format(grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Tasks\n",
    "Now that we've seen how to use GridSearchCV, it's your turn to use it on the Naive Bayes model. Pick an evaluation metric to use for cross validation like F1 score, ROC AUC score, etc. In the spam classification setting, do you think it makes sense to optimize for higher precision? recall? f1 score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Write some code to use GridSearchCV to learn the best alpha parameter for your Multinomial Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Now that you've found the optimal alpha using cross validation over the training set, evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further Questions:\n",
    "* Naive Bayes has a seemingly stringent independence assumption. Do you think the variables in this email classification problem(word count vectors) are really independent? Does this matter? Why/why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "Two weeks ago, we played around with decision trees. In general, practictioners almost never use decision trees over Random Forests. Recall from lecture, that Random Forests takes some number of subsamples of the data:\n",
    "$S_1, S_2, ..., S_k \\subseteq \\{(x^{(i)}, y^{(i)}\\}_{i=0}^N$. On each subsample of the data, $S_i$, the Random Forests algorithm trains a decision tree on $S_i$.\n",
    "\n",
    "At classification time, when we get a datapoint $x$, the Random Forests classifier runs $x$ through decision trees $D_1, D_2, ..., D_k$, which produces a \"probability\" estimate of each class:\n",
    "\n",
    "$$P(y = 1 | x) = \\frac{\\text{number of decision trees that classified $x$ as 1}}{k}$$\n",
    "\n",
    "$$P(y = 0 | x) = \\frac{\\text{number of decision trees that classified $x$ as 1}}{k}$$\n",
    "\n",
    "Then from there, we get a class prediction by picking $y = 1$ if $P(y = 1 | x) > T$, for some threshold probability $T$.\n",
    "\n",
    "\n",
    "Instead of using the spam dataset to try out Random Forests, we'll re-use the credit loan data from Assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do whatever data cleaning, data imputing, etc you need to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Parameters\n",
    "\n",
    "The Random Forest parameters are more or less the same as those for Decision Trees. The only additional parameter is n_estimators, which is the number of decision trees to grow for the algorithm.\n",
    "* n_estimators : integer, the number of trees in the forest.\n",
    "* criterion : string,  “gini” or “entropy” \n",
    "* max_features : int, float, string or None, the number of features to consider when looking for the best split\n",
    "* max_depth : integer, the maximum depth of the tree\n",
    "* min_samples_split : int, float, the minimum number of samples required to split an internal node:\n",
    "* min_samples_leaf : int, float, optional (default=1), the minimum number of samples required to be at a leaf node:\n",
    "* min_weight_fraction_leaf : float, optional \n",
    "* max_leaf_nodes : int or None, optional (default=None)\n",
    "\n",
    "0) What do you think will be the effect of increasing the n\\_estimators parameter? If n_estimators = 1, is this any different to using a decision tree classifier(assuming you set all the other parameters like min_sample_split, max_depth, etc the same way in both cases)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of parameters here. Imagine writing a 8x nested for-loop to find all the combinations of parameters for a RandomForests model. That doesn't sound too fun right? Good thing we have GridSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Pick an evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Pick a few values for each of the random forest parameters for the param\\_grid. Note that this Grid Search can be a lot more time consuming than the one you did for Naive Bayes because each Random Forest has to train multiple decision trees for each combination of parameters in your param_grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# param settings to try out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Write some code to do the grid search for Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do the grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Random Forests has a [feature importances](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.feature_importances_) attribute. What features are the most important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Rerun the classifier with a different scoring metric. Do the feature importances change? Which features are most important now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* https://etav.github.io/projects/spam_message_classifier_naive_bayes.html\n",
    "* http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "* http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
